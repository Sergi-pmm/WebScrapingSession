{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping en p√°ginas acad√©micas\n",
    "\n",
    "En este notebook aprenderemos a extraer informaci√≥n de dos fuentes acad√©micas muy populares:\n",
    "\n",
    "- **arXiv.org** ‚Üí repositorio de art√≠culos cient√≠ficos de acceso abierto\n",
    "- **Semantic Scholar** ‚Üí buscador acad√©mico con API p√∫blica\n",
    "\n",
    "Veremos **tres enfoques diferentes** para obtener datos:\n",
    "\n",
    "| Enfoque | Fuente | Herramienta |\n",
    "|---------|--------|-------------|\n",
    "| Scraping HTML | P√°gina de art√≠culo en arXiv | `bs4` + `requests` |\n",
    "| Parsing XML | API de arXiv | `bs4` + `requests` |\n",
    "| Consumo de API REST | API de Semantic Scholar | `requests` + JSON |\n",
    "\n",
    "**Librer√≠as necesarias:** `requests`, `beautifulsoup4`, `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 1: Scraping HTML en arXiv\n",
    "\n",
    "### ¬øQu√© es arXiv?\n",
    "\n",
    "**arXiv.org** es un repositorio de acceso abierto con m√°s de 2 millones de art√≠culos de f√≠sica, matem√°ticas, inform√°tica, biolog√≠a, etc. Los investigadores publican aqu√≠ sus trabajos antes (o en paralelo) de la revisi√≥n por pares.\n",
    "\n",
    "### Objetivo\n",
    "Vamos a acceder a la **p√°gina de un art√≠culo** en arXiv y extraer:\n",
    "- T√≠tulo\n",
    "- Autores\n",
    "- Resumen (abstract)\n",
    "- Categor√≠as\n",
    "- Fecha de publicaci√≥n\n",
    "- Enlace al PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Descargar la p√°gina de un art√≠culo\n",
    "\n",
    "Cada art√≠culo en arXiv tiene una URL tipo `https://arxiv.org/abs/XXXX.XXXXX`.  \n",
    "Vamos a usar un art√≠culo famoso sobre **Attention** (la base de los Transformers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL del art√≠culo \"Attention Is All You Need\"\n",
    "url = \"https://arxiv.org/abs/1706.03762\"\n",
    "\n",
    "# Hacemos la petici√≥n HTTP\n",
    "respuesta = requests.get(url)\n",
    "print(f\"Status code: {respuesta.status_code}\")\n",
    "print(f\"Tama√±o de la respuesta: {len(respuesta.text):,} caracteres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parseamos el HTML con BeautifulSoup\n",
    "soup = BeautifulSoup(respuesta.text, \"html.parser\")\n",
    "\n",
    "# Veamos el t√≠tulo de la p√°gina para confirmar que todo va bien\n",
    "print(soup.title.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Extraer el t√≠tulo del art√≠culo\n",
    "\n",
    "Si inspeccionamos el HTML de arXiv (bot√≥n derecho ‚Üí \"Inspeccionar\"), veremos que el t√≠tulo del paper est√° dentro de una etiqueta `<h1 class=\"title mathjax\">`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El t√≠tulo est√° en un <h1> con clase \"title mathjax\"\n",
    "titulo_tag = soup.find(\"h1\", class_=\"title mathjax\")\n",
    "print(\"Tag completo:\", titulo_tag)\n",
    "print()\n",
    "\n",
    "# El tag contiene \"Title:\" como texto previo, lo quitamos\n",
    "titulo = titulo_tag.text.replace(\"Title:\", \"\").strip()\n",
    "print(\"T√≠tulo limpio:\", titulo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Extraer los autores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los autores est√°n en <div class=\"authors\">\n",
    "autores_div = soup.find(\"div\", class_=\"authors\")\n",
    "\n",
    "# Cada autor es un enlace <a> dentro de ese div\n",
    "autores = [a.text.strip() for a in autores_div.find_all(\"a\")]\n",
    "print(f\"N√∫mero de autores: {len(autores)}\")\n",
    "print(\"Autores:\", \", \".join(autores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Extraer el abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El abstract est√° en <blockquote class=\"abstract mathjax\">\n",
    "abstract_tag = soup.find(\"blockquote\", class_=\"abstract mathjax\")\n",
    "\n",
    "# Quitamos el prefijo \"Abstract:\" y limpiamos espacios\n",
    "abstract = abstract_tag.text.replace(\"Abstract:\", \"\").strip()\n",
    "print(abstract[:300], \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Extraer categor√≠as, fecha y enlace al PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categor√≠as: est√°n en <span class=\"primary-subject\">\n",
    "categoria = soup.find(\"span\", class_=\"primary-subject\").text.strip()\n",
    "print(\"Categor√≠a principal:\", categoria)\n",
    "\n",
    "# Fecha: est√° en el div \"submission-history\"\n",
    "historial = soup.find(\"div\", class_=\"submission-history\")\n",
    "# La primera fecha suele ser la de env√≠o original\n",
    "fecha_texto = historial.text.strip()\n",
    "print(\"\\nHistorial de env√≠o (primeras l√≠neas):\")\n",
    "for linea in fecha_texto.split(\"\\n\")[:5]:\n",
    "    if linea.strip():\n",
    "        print(\" \", linea.strip())\n",
    "\n",
    "# Enlace al PDF: se construye cambiando /abs/ por /pdf/\n",
    "pdf_url = url.replace(\"/abs/\", \"/pdf/\")\n",
    "print(f\"\\nEnlace al PDF: {pdf_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Extraer varios art√≠culos y crear un DataFrame\n",
    "\n",
    "Ahora vamos a hacer lo mismo con **varios art√≠culos** cl√°sicos de Deep Learning y guardar los resultados en un DataFrame de pandas.\n",
    "\n",
    "> **Buenas pr√°cticas:** Entre petici√≥n y petici√≥n a√±adimos un `time.sleep()` para no saturar el servidor. arXiv pide un m√≠nimo de 3 segundos entre peticiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de art√≠culos famosos de Deep Learning (sus IDs en arXiv)\n",
    "articulos_ids = [\n",
    "    \"1706.03762\",  # Attention Is All You Need\n",
    "    \"1810.04805\",  # BERT\n",
    "    \"2005.14165\",  # GPT-3\n",
    "    \"1512.03385\",  # ResNet\n",
    "    \"1406.2661\",  # GANs\n",
    "]\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for arxiv_id in articulos_ids:\n",
    "    url = f\"https://arxiv.org/abs/{arxiv_id}\"\n",
    "    print(f\"Descargando: {arxiv_id}...\", end=\" \")\n",
    "    \n",
    "    respuesta = requests.get(url)\n",
    "    \n",
    "    if respuesta.status_code != 200:\n",
    "        print(f\"ERROR ({respuesta.status_code})\")\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(respuesta.text, \"html.parser\")\n",
    "    \n",
    "    # Extraemos los campos\n",
    "    titulo = soup.find(\"h1\", class_=\"title mathjax\").text.replace(\"Title:\", \"\").strip()\n",
    "    autores = [a.text.strip() for a in soup.find(\"div\", class_=\"authors\").find_all(\"a\")]\n",
    "    abstract = soup.find(\"blockquote\", class_=\"abstract mathjax\").text.replace(\"Abstract:\", \"\").strip()\n",
    "    categoria = soup.find(\"span\", class_=\"primary-subject\").text.strip()\n",
    "    \n",
    "    resultados.append({\n",
    "        \"arxiv_id\": arxiv_id,\n",
    "        \"titulo\": titulo,\n",
    "        \"autores\": \", \".join(autores),\n",
    "        \"num_autores\": len(autores),\n",
    "        \"abstract\": abstract[:200] + \"...\",\n",
    "        \"categoria\": categoria,\n",
    "        \"url_pdf\": f\"https://arxiv.org/pdf/{arxiv_id}\",\n",
    "    })\n",
    "    \n",
    "    print(f\"OK ‚Üí {titulo[:50]}...\")\n",
    "    time.sleep(3)  # Respetar el rate limit de arXiv\n",
    "\n",
    "print(f\"\\n{len(resultados)} art√≠culos extra√≠dos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el DataFrame\n",
    "df_arxiv = pd.DataFrame(resultados)\n",
    "df_arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 2: API XML de arXiv\n",
    "\n",
    "arXiv ofrece una **API oficial** que devuelve resultados en formato **XML (Atom)**. Es la forma \"correcta\" de hacer b√∫squedas masivas.\n",
    "\n",
    "**Ventajas frente al scraping HTML:**\n",
    "- No dependemos de que cambien el dise√±o de la web\n",
    "- Datos m√°s estructurados y limpios\n",
    "- Permitido expl√≠citamente por arXiv\n",
    "\n",
    "**¬°Y podemos seguir usando BeautifulSoup!** porque bs4 tambi√©n parsea XML.\n",
    "\n",
    "Documentaci√≥n: https://info.arxiv.org/help/api/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Buscar art√≠culos por tema\n",
    "\n",
    "La URL de la API tiene esta estructura:\n",
    "```\n",
    "http://export.arxiv.org/api/query?search_query=TERMINO&start=0&max_results=10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscamos art√≠culos sobre \"large language models\"\n",
    "termino = \"large language models\"\n",
    "max_resultados = 4\n",
    "\n",
    "url_api = f\"http://export.arxiv.org/api/query?search_query=all:{termino}&start=0&max_results={max_resultados}\"\n",
    "print(f\"URL de la API: {url_api}\")\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "respuesta = requests.get(url_api)\n",
    "print(f\"Status code: {respuesta.status_code}\")\n",
    "print(f\"Tipo de contenido: {respuesta.headers.get('Content-Type', 'desconocido')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos un trozo del XML que nos devuelve\n",
    "print(respuesta.text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Parsear el XML con BeautifulSoup\n",
    "\n",
    "El XML devuelto sigue el formato **Atom**. Cada art√≠culo est√° dentro de una etiqueta `<entry>` con sub-etiquetas como `<title>`, `<summary>`, `<author>`, etc.\n",
    "\n",
    "> üí° **Nota:** Usamos el parser `\"xml\"` en lugar de `\"html.parser\"`. Para que funcione necesitamos tener instalado `lxml` (`pip install lxml`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parseamos el XML\n",
    "soup_xml = BeautifulSoup(respuesta.text, \"xml\")\n",
    "\n",
    "# Cada art√≠culo es un <entry>\n",
    "entradas = soup_xml.find_all(\"entry\")\n",
    "print(f\"Art√≠culos encontrados: {len(entradas)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos la estructura de la primera entrada\n",
    "primera = entradas[1]\n",
    "print(primera.prettify()[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos los datos de cada entrada\n",
    "resultados_api = []\n",
    "\n",
    "for entry in entradas:\n",
    "    # ID de arXiv (viene como URL, extraemos solo el ID)\n",
    "    arxiv_url = entry.find(\"id\").text\n",
    "    arxiv_id = arxiv_url.split(\"/abs/\")[-1]\n",
    "    \n",
    "    # T√≠tulo (a veces viene con saltos de l√≠nea, los limpiamos)\n",
    "    titulo = entry.find(\"title\").text.strip().replace(\"\\n\", \" \")\n",
    "    \n",
    "    # Autores: cada uno est√° en su propio <author><name>...</name></author>\n",
    "    autores = [author.find(\"name\").text for author in entry.find_all(\"author\")]\n",
    "    \n",
    "    # Resumen\n",
    "    resumen = entry.find(\"summary\").text.strip().replace(\"\\n\", \" \")\n",
    "    \n",
    "    # Fecha de publicaci√≥n\n",
    "    fecha = entry.find(\"published\").text[:10]  # Solo YYYY-MM-DD\n",
    "    \n",
    "    # Categor√≠a principal\n",
    "    categoria = entry.find(\"category\")[\"term\"]\n",
    "    \n",
    "    # Enlace al PDF\n",
    "    enlaces = entry.find_all(\"link\")\n",
    "    pdf_link = \"\"\n",
    "    for enlace in enlaces:\n",
    "        if enlace.get(\"title\") == \"pdf\":\n",
    "            pdf_link = enlace[\"href\"]\n",
    "            break\n",
    "    \n",
    "    resultados_api.append({\n",
    "        \"arxiv_id\": arxiv_id,\n",
    "        \"titulo\": titulo,\n",
    "        \"primer_autor\": autores[0] if autores else \"\",\n",
    "        \"num_autores\": len(autores),\n",
    "        \"fecha\": fecha,\n",
    "        \"categoria\": categoria,\n",
    "        \"resumen\": resumen[:150] + \"...\",\n",
    "        \"pdf\": pdf_link,\n",
    "    })\n",
    "\n",
    "print(f\" {len(resultados_api)} art√≠culos procesados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_arxiv_api = pd.DataFrame(resultados_api)\n",
    "df_arxiv_api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Ejemplo: cambiar el t√©rmino de b√∫squeda\n",
    "\n",
    "Puedes probar con otros temas. Algunos ejemplos:\n",
    "- `\"web scraping\"` \n",
    "- `\"natural language processing\"`\n",
    "- `\"climate change prediction\"`\n",
    "- `\"quantum computing\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba: cambia el t√©rmino aqu√≠ y ejecuta esta celda\n",
    "termino = \"web scraping\"  # ‚Üê Cambia este valor\n",
    "\n",
    "url_api = f\"http://export.arxiv.org/api/query?search_query=all:{termino}&start=0&max_results=5&sortBy=relevance\"\n",
    "respuesta = requests.get(url_api)\n",
    "soup_xml = BeautifulSoup(respuesta.text, \"xml\")\n",
    "\n",
    "for i, entry in enumerate(soup_xml.find_all(\"entry\"), 1):\n",
    "    titulo = entry.find(\"title\").text.strip().replace(\"\\n\", \" \")\n",
    "    fecha = entry.find(\"published\").text[:10]\n",
    "    print(f\"{i}. [{fecha}] {titulo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 3: API de Semantic Scholar\n",
    "\n",
    "### ¬øQu√© es Semantic Scholar?\n",
    "\n",
    "**Semantic Scholar** (semanticscholar.org) es un buscador acad√©mico creado por el Allen Institute for AI. Indexa m√°s de 200 millones de papers de todas las disciplinas.\n",
    "\n",
    "### ¬øPor qu√© usar su API en vez de scraping?\n",
    "\n",
    "Semantic Scholar ofrece una **API REST gratuita** que devuelve datos en **JSON**. Cuando una web ofrece API, es mejor usarla que hacer scraping porque:\n",
    "\n",
    "- Los datos vienen **limpios y estructurados**\n",
    "- No se rompe si cambian el dise√±o de la web\n",
    "- Es el m√©todo que la web quiere que uses\n",
    "- No necesitas parsear HTML\n",
    "\n",
    "> Aqu√≠ ya no necesitamos `bs4` porque el JSON se parsea directamente con Python. Es importante saber cu√°ndo **NO** hace falta scraping.\n",
    "\n",
    "Documentaci√≥n: https://api.semanticscholar.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Buscar papers por palabra clave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL base de la API de Semantic Scholar\n",
    "BASE_URL = \"https://api.semanticscholar.org/graph/v1\"\n",
    "\n",
    "# Buscamos papers sobre \"web scraping\"\n",
    "termino = \"web scraping\"\n",
    "campos = \"title,year,authors,citationCount,openAccessPdf,externalIds\"\n",
    "\n",
    "url = f\"{BASE_URL}/paper/search\"\n",
    "params = {\n",
    "    \"query\": termino,\n",
    "    \"limit\": 10,\n",
    "    \"fields\": campos,\n",
    "}\n",
    "\n",
    "respuesta = requests.get(url, params=params)\n",
    "print(f\"Status code: {respuesta.status_code}\")\n",
    "print(f\"URL final: {respuesta.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La respuesta es JSON ‚Üí lo convertimos a diccionario de Python\n",
    "datos = respuesta.json()\n",
    "\n",
    "# Veamos las claves del resultado\n",
    "print(\"Claves del JSON:\", list(datos.keys()))\n",
    "print(f\"Total de resultados: {datos.get('total', 'N/A')}\")\n",
    "print(f\"Papers devueltos: {len(datos.get('data', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veamos la estructura del primer resultado\n",
    "import json\n",
    "print(json.dumps(datos[\"data\"][0], indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraemos los datos relevantes de cada paper\n",
    "resultados_ss = []\n",
    "\n",
    "for paper in datos[\"data\"]:\n",
    "    # Nombres de los autores\n",
    "    autores = [a[\"name\"] for a in paper.get(\"authors\", [])]\n",
    "    \n",
    "    # Enlace al PDF (si existe acceso abierto)\n",
    "    pdf_info = paper.get(\"openAccessPdf\")\n",
    "    pdf_url = pdf_info[\"url\"] if pdf_info else \"No disponible\"\n",
    "    \n",
    "    # DOI (si existe)\n",
    "    external = paper.get(\"externalIds\", {})\n",
    "    doi = external.get(\"DOI\", \"\")\n",
    "    \n",
    "    resultados_ss.append({\n",
    "        \"titulo\": paper[\"title\"],\n",
    "        \"a√±o\": paper.get(\"year\", \"\"),\n",
    "        \"primer_autor\": autores[0] if autores else \"\",\n",
    "        \"num_autores\": len(autores),\n",
    "        \"citas\": paper.get(\"citationCount\", 0),\n",
    "        \"doi\": doi,\n",
    "        \"pdf\": pdf_url,\n",
    "    })\n",
    "\n",
    "print(f\"{len(resultados_ss)} papers procesados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ss = pd.DataFrame(resultados_ss)\n",
    "\n",
    "# Ordenamos por n√∫mero de citas (los m√°s citados primero)\n",
    "df_ss = df_ss.sort_values(\"citas\", ascending=False).reset_index(drop=True)\n",
    "df_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Obtener detalles de un paper concreto\n",
    "\n",
    "Si conocemos el ID de un paper (DOI, arXiv ID, etc.), podemos pedir m√°s detalles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pedimos detalles del paper \"Attention Is All You Need\" usando su arXiv ID\n",
    "paper_id = \"arXiv:1706.03762\"\n",
    "campos = \"title,year,authors,abstract,citationCount,influentialCitationCount,tldr\"\n",
    "\n",
    "url = f\"{BASE_URL}/paper/{paper_id}\"\n",
    "respuesta = requests.get(url, params={\"fields\": campos})\n",
    "\n",
    "paper = respuesta.json()\n",
    "\n",
    "print(f\"T√≠tulo: {paper['title']}\")\n",
    "print(f\"A√±o: {paper['year']}\")\n",
    "print(f\"Autores: {len(paper['authors'])}\")\n",
    "print(f\"Citas totales: {paper['citationCount']:,}\")\n",
    "print(f\"Citas influyentes: {paper['influentialCitationCount']:,}\")\n",
    "\n",
    "# TLDR = resumen autom√°tico generado por Semantic Scholar\n",
    "tldr = paper.get(\"tldr\")\n",
    "if tldr:\n",
    "    print(f\"\\nTLDR: {tldr['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Buscar papers de un autor\n",
    "\n",
    "Tambi√©n podemos buscar por nombre de autor y ver sus publicaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscamos al autor\n",
    "nombre_autor = \"Yoshua Bengio\"\n",
    "\n",
    "url = f\"{BASE_URL}/author/search\"\n",
    "respuesta = requests.get(url, params={\"query\": nombre_autor, \"limit\": 1})\n",
    "datos_autor = respuesta.json()\n",
    "\n",
    "# Obtenemos su ID\n",
    "autor_id = datos_autor[\"data\"][0][\"authorId\"]\n",
    "autor_nombre = datos_autor[\"data\"][0][\"name\"]\n",
    "print(f\"Autor encontrado: {autor_nombre} (ID: {autor_id})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora pedimos sus papers m√°s citados\n",
    "url = f\"{BASE_URL}/author/{autor_id}/papers\"\n",
    "params = {\n",
    "    \"fields\": \"title,year,citationCount\",\n",
    "    \"limit\": 10,\n",
    "}\n",
    "\n",
    "respuesta = requests.get(url, params=params)\n",
    "papers_autor = respuesta.json()[\"data\"]\n",
    "\n",
    "# Convertimos a DataFrame y ordenamos por citas\n",
    "df_autor = pd.DataFrame(papers_autor)\n",
    "df_autor = df_autor.sort_values(\"citationCount\", ascending=False).reset_index(drop=True)\n",
    "df_autor[[\"title\", \"year\", \"citationCount\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resumen y comparaci√≥n\n",
    "\n",
    "| | Scraping HTML | API XML (arXiv) | API REST (Semantic Scholar) |\n",
    "|---|---|---|---|\n",
    "| **Herramientas** | `requests` + `bs4` | `requests` + `bs4` (parser XML) | `requests` + `json` |\n",
    "| **Formato respuesta** | HTML | XML (Atom) | JSON |\n",
    "| **Fragilidad** | Alta (si cambia el dise√±o, se rompe) | Baja (formato estable) | Baja (formato estable) |\n",
    "| **Velocidad** | Hay que parsear mucho HTML | R√°pido | Muy r√°pido |\n",
    "| **Cu√°ndo usarlo** | Cuando NO hay API | Cuando hay API que devuelve XML | Cuando hay API REST moderna |\n",
    "| **¬øNecesita bs4?** | S√≠ | S√≠ (parser XML) | No |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
